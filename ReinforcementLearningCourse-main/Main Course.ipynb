{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines3.readthedocs.io/en/master/guide/rl.html\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in d:\\anaconda3\\envs\\rl\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: torch>=1.11 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (1.12.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (1.23.1)\n",
      "Requirement already satisfied: cloudpickle in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (1.6.0)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (3.5.2)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (4.13.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (1.5.2)\n",
      "Requirement already satisfied: gym==0.21 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
      "Requirement already satisfied: rich in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (12.6.0)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (2.11.0)\n",
      "Requirement already satisfied: pillow in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (9.2.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: ale-py==0.7.4 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (0.7.4)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (4.64.1)\n",
      "Requirement already satisfied: psutil in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from stable-baselines3[extra]) (5.9.4)\n",
      "Requirement already satisfied: importlib-resources in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.10.1)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.1.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.28.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.51.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.15.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: typing_extensions in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (4.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2022.7)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.13.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from rich->stable-baselines3[extra]) (0.9.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.1)\n",
      "Requirement already satisfied: libtorrent in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda3\\envs\\rl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_state = [-0.01992185 -0.20394045  0.0479903   0.34802204], reward = 1.0, done = False\n",
      "n_state = [-0.02400066 -0.39971095  0.05495074  0.65544343], reward = 1.0, done = False\n",
      "n_state = [-0.03199488 -0.20539537  0.06805961  0.3805574 ], reward = 1.0, done = False\n",
      "n_state = [-0.03610279 -0.40141445  0.07567076  0.69389933], reward = 1.0, done = False\n",
      "n_state = [-0.04413108 -0.2074192   0.08954874  0.4259652 ], reward = 1.0, done = False\n",
      "n_state = [-0.04827946 -0.40368786  0.09806804  0.7454807 ], reward = 1.0, done = False\n",
      "n_state = [-0.05635322 -0.2100461   0.11297766  0.4851995 ], reward = 1.0, done = False\n",
      "n_state = [-0.06055414 -0.4065659   0.12268165  0.8112448 ], reward = 1.0, done = False\n",
      "n_state = [-0.06868546 -0.21331897  0.13890655  0.55953103], reward = 1.0, done = False\n",
      "n_state = [-0.07295184 -0.02039196  0.15009716  0.31363523], reward = 1.0, done = False\n",
      "n_state = [-0.07335968 -0.21729782  0.15636986  0.64963627], reward = 1.0, done = False\n",
      "n_state = [-0.07770564 -0.4142124   0.16936259  0.98718995], reward = 1.0, done = False\n",
      "n_state = [-0.08598989 -0.2217131   0.18910639  0.7521272 ], reward = 1.0, done = False\n",
      "n_state = [-0.09042414 -0.02963192  0.20414893  0.52440834], reward = 1.0, done = False\n",
      "n_state = [-0.09101678  0.16212063  0.2146371   0.30235624], reward = 1.0, done = True\n",
      "Episode:1 Score:15.0\n",
      "n_state = [-0.00192208 -0.24215314 -0.02153265  0.2452139 ], reward = 1.0, done = False\n",
      "n_state = [-0.00676514 -0.04673037 -0.01662837 -0.05418242], reward = 1.0, done = False\n",
      "n_state = [-0.00769975  0.14862601 -0.01771202 -0.352065  ], reward = 1.0, done = False\n",
      "n_state = [-0.00472723  0.3439953  -0.02475332 -0.65028   ], reward = 1.0, done = False\n",
      "n_state = [ 0.00215268  0.14922673 -0.03775892 -0.3654934 ], reward = 1.0, done = False\n",
      "n_state = [ 0.00513721  0.34486437 -0.04506879 -0.66983914], reward = 1.0, done = False\n",
      "n_state = [ 0.0120345   0.540583   -0.05846557 -0.97636473], reward = 1.0, done = False\n",
      "n_state = [ 0.02284616  0.7364383  -0.07799286 -1.2868247 ], reward = 1.0, done = False\n",
      "n_state = [ 0.03757493  0.93246126 -0.10372936 -1.6028721 ], reward = 1.0, done = False\n",
      "n_state = [ 0.05622415  0.73870873 -0.1357868  -1.3442472 ], reward = 1.0, done = False\n",
      "n_state = [ 0.07099833  0.5455307  -0.16267174 -1.0969462 ], reward = 1.0, done = False\n",
      "n_state = [ 0.08190894  0.74237674 -0.18461066 -1.435932  ], reward = 1.0, done = False\n",
      "n_state = [ 0.09675647  0.5499478  -0.21332932 -1.206161  ], reward = 1.0, done = True\n",
      "Episode:2 Score:13.0\n",
      "n_state = [-0.02667017 -0.18038045 -0.00851958  0.24538201], reward = 1.0, done = False\n",
      "n_state = [-0.03027778 -0.37537968 -0.00361194  0.5353655 ], reward = 1.0, done = False\n",
      "n_state = [-0.03778538 -0.18020713  0.00709537  0.24154669], reward = 1.0, done = False\n",
      "n_state = [-0.04138952 -0.37542972  0.0119263   0.5364592 ], reward = 1.0, done = False\n",
      "n_state = [-0.04889811 -0.57071733  0.02265549  0.832876  ], reward = 1.0, done = False\n",
      "n_state = [-0.06031246 -0.37591216  0.03931301  0.5474034 ], reward = 1.0, done = False\n",
      "n_state = [-0.0678307  -0.18136393  0.05026107  0.26736158], reward = 1.0, done = False\n",
      "n_state = [-0.07145798 -0.37716582  0.05560831  0.57546407], reward = 1.0, done = False\n",
      "n_state = [-0.0790013  -0.5730214   0.06711759  0.88513374], reward = 1.0, done = False\n",
      "n_state = [-0.09046172 -0.76898724  0.08482026  1.1981385 ], reward = 1.0, done = False\n",
      "n_state = [-0.10584147 -0.5750591   0.10878303  0.93319935], reward = 1.0, done = False\n",
      "n_state = [-0.11734265 -0.77146727  0.12744702  1.2579893 ], reward = 1.0, done = False\n",
      "n_state = [-0.132772  -0.5781856  0.1526068  1.0077862], reward = 1.0, done = False\n",
      "n_state = [-0.14433572 -0.77497894  0.17276253  1.344238  ], reward = 1.0, done = False\n",
      "n_state = [-0.1598353  -0.582399    0.19964729  1.1102083 ], reward = 1.0, done = False\n",
      "n_state = [-0.17148326 -0.779503    0.22185145  1.4582986 ], reward = 1.0, done = True\n",
      "Episode:3 Score:16.0\n",
      "n_state = [ 0.02033022  0.14843456  0.02686132 -0.318866  ], reward = 1.0, done = False\n",
      "n_state = [ 0.02329892  0.34316385  0.020484   -0.6029583 ], reward = 1.0, done = False\n",
      "n_state = [ 0.03016219  0.5379934   0.00842484 -0.88911945], reward = 1.0, done = False\n",
      "n_state = [ 0.04092206  0.34275815 -0.00935755 -0.59380007], reward = 1.0, done = False\n",
      "n_state = [ 0.04777722  0.5380098  -0.02123355 -0.88941586], reward = 1.0, done = False\n",
      "n_state = [ 0.05853742  0.34318233 -0.03902187 -0.60348266], reward = 1.0, done = False\n",
      "n_state = [ 0.06540107  0.5388277  -0.05109153 -0.90819687], reward = 1.0, done = False\n",
      "n_state = [ 0.07617762  0.3444332  -0.06925546 -0.63199997], reward = 1.0, done = False\n",
      "n_state = [ 0.08306628  0.15034235 -0.08189546 -0.36190644], reward = 1.0, done = False\n",
      "n_state = [ 0.08607313  0.34652704 -0.08913359 -0.67924756], reward = 1.0, done = False\n",
      "n_state = [ 0.09300368  0.5427667  -0.10271854 -0.99860907], reward = 1.0, done = False\n",
      "n_state = [ 0.10385901  0.3491567  -0.12269072 -0.739871  ], reward = 1.0, done = False\n",
      "n_state = [ 0.11084214  0.15592314 -0.13748814 -0.48818013], reward = 1.0, done = False\n",
      "n_state = [ 0.1139606   0.3526898  -0.14725174 -0.8208392 ], reward = 1.0, done = False\n",
      "n_state = [ 0.1210144   0.15985656 -0.16366853 -0.57785535], reward = 1.0, done = False\n",
      "n_state = [ 0.12421153  0.3568483  -0.17522563 -0.9172981 ], reward = 1.0, done = False\n",
      "n_state = [ 0.13134849  0.55385095 -0.1935716  -1.2595302 ], reward = 1.0, done = False\n",
      "n_state = [ 0.14242552  0.3616589  -0.2187622  -1.0331817 ], reward = 1.0, done = True\n",
      "Episode:4 Score:18.0\n",
      "n_state = [ 0.00270087 -0.20624284 -0.04180131  0.293811  ], reward = 1.0, done = False\n",
      "n_state = [-0.00142399 -0.01055062 -0.03592509 -0.01175691], reward = 1.0, done = False\n",
      "n_state = [-0.001635   -0.20513943 -0.03616023  0.26937827], reward = 1.0, done = False\n",
      "n_state = [-0.00573779 -0.3997272  -0.03077266  0.5504403 ], reward = 1.0, done = False\n",
      "n_state = [-0.01373233 -0.5944037  -0.01976386  0.8332711 ], reward = 1.0, done = False\n",
      "n_state = [-0.0256204  -0.78925014 -0.00309844  1.1196734 ], reward = 1.0, done = False\n",
      "n_state = [-0.04140541 -0.9843313   0.01929503  1.4113828 ], reward = 1.0, done = False\n",
      "n_state = [-0.06109203 -1.179687    0.04752269  1.7100344 ], reward = 1.0, done = False\n",
      "n_state = [-0.08468577 -1.3753217   0.08172338  2.0171213 ], reward = 1.0, done = False\n",
      "n_state = [-0.11219221 -1.1811372   0.1220658   1.7508168 ], reward = 1.0, done = False\n",
      "n_state = [-0.13581495 -1.3774154   0.15708214  2.0788417 ], reward = 1.0, done = False\n",
      "n_state = [-0.16336326 -1.5737422   0.19865897  2.4156983 ], reward = 1.0, done = False\n",
      "n_state = [-0.1948381  -1.3808211   0.24697293  2.1900306 ], reward = 1.0, done = True\n",
      "Episode:5 Score:13.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        print(f'n_state = {n_state}, reward = {reward}, done = {done}')\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding The Environment\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1974113e+00, -1.4069810e+38, -3.6003578e-01, -2.0436207e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1937 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1197        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009211546 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 58.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1048        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009840878 |\n",
      "|    clip_fraction        | 0.0762      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 988          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081472825 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.239        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0221      |\n",
      "|    value_loss           | 54.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 958         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008058009 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 76.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006290865 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013115541 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.596       |\n",
      "|    explained_variance   | 0.459        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.37         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    value_loss           | 46.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 912         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003217645 |\n",
      "|    clip_fraction        | 0.0168      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.32        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 905         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003412663 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490325 |\n",
      "|    clip_fraction        | 0.0245      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.995       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x22b1d5c30d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\Saved Models\\PPO_model\n"
     ]
    }
   ],
   "source": [
    "print(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:276: UserWarning: Path 'Training\\Saved Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adding a callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
