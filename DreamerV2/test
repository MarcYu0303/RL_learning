import torch
import torch.nn as nn

if __name__ == '__main__':
    batch_size = 64
    image_embedding_mlp = nn.Sequential( # x --> x_embedding
            #3,64,64
            nn.Conv2d(3, 32, 3, padding=1, stride=2), #32,32,32
            nn.ELU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1, stride=2), #64,16,16
            nn.ELU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1, stride=2), #128, 8, 8
            nn.ELU(inplace=True),
            nn.Conv2d(128, 256, 3, padding=1, stride=2), #256, 4, 4
            nn.ELU(inplace=True),
            nn.Conv2d(256, 512, 4), #512, 1, 1
            nn.ELU(inplace=True)
    )
    representation_model_mlp = nn.Sequential( # x + h --> z
            nn.Linear(512+512, 1024),
            nn.ELU(inplace=True),
            nn.Linear(1024, 1024),
            nn.ELU(inplace=True),
            nn.Linear(1024, 1024),
        )
    
    '-------'
    h = torch.zeros((1, 512))
    x_t = torch.rand(3, 64, 64)
    embedding = image_embedding_mlp(x_t)
    print(f'x_t shape: {x_t.shape}')
    print(f'x embedding shape: {embedding.shape}')
    print('------')
    
    embedding = embedding.view(-1, 512)
    print(f'h shape: {h.shape}')
    print(f'embedding reshape size: {embedding.shape}')
    embedding = torch.cat((h, embedding), dim=1)
    print(f'embedding cat size: {embedding.shape}')
    print('------')
    
    z_logits = representation_model_mlp(embedding)
    print(f'z_logic shape: {z_logits.shape}')
    z_sample = torch.distributions.one_hot_categorical.OneHotCategorical(
            logits=z_logits.reshape(-1, 32, 32)
        ).sample()
    print(f'z_sample shape: {z_sample.shape}')
    print(z_sample)
    
    random_action_dist = torch.distributions.one_hot_categorical.OneHotCategorical(
        torch.ones((1,5))
    )
    print(random_action_dist)